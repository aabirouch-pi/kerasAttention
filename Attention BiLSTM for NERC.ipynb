{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Attention mechanisms to an LSTM network \n",
    "\n",
    "We will be working with the CONLL 2003 dataset, annotated for the task of Named Entity Recognition.\n",
    "\n",
    "The objetive of this notebook is to build a prototype LSTM for sequence labeling, and apply a very simple attention mechanisms before the recurrent layer. The base model is inpired in [this work](https://www.kaggle.com/gagandeep16/ner-using-bidirectional-lstm), by GaganBhatia. Most of the explanations of the code is in the accompaning slides.\n",
    "\n",
    "You can find two sample datasets directly hosted at UNC, [one](https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.csv) used by the original Kaggle notebook (150M) and a [smaller one](https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.sample.csv) just to play with (14M).\n",
    "\n",
    "\n",
    "Once the model is trained, we show the attention score for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset and extracting sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milagro/miniconda2/envs/am_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dataset = pandas.read_csv(\"../ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False,\n",
    "                          usecols=['sentence_idx', 'word', 'pos', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050796, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VBP</td>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VBN</td>\n",
       "      <td>1</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>through</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>London</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TO</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>protest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DT</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>war</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>demand</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DT</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "      <td>British</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>troops</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pos sentence_idx           word    tag\n",
       "0   NNS            1      Thousands      O\n",
       "1    IN            1             of      O\n",
       "2   NNS            1  demonstrators      O\n",
       "3   VBP            1           have      O\n",
       "4   VBN            1        marched      O\n",
       "5    IN            1        through      O\n",
       "6   NNP            1         London  B-geo\n",
       "7    TO            1             to      O\n",
       "8    VB            1        protest      O\n",
       "9    DT            1            the      O\n",
       "10   NN            1            war      O\n",
       "11   IN            1             in      O\n",
       "12  NNP            1           Iraq  B-geo\n",
       "13   CC            1            and      O\n",
       "14   VB            1         demand      O\n",
       "15   DT            1            the      O\n",
       "16   NN            1     withdrawal      O\n",
       "17   IN            1             of      O\n",
       "18   JJ            1        British  B-gpe\n",
       "19  NNS            1         troops      O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentenceFactory(object):\n",
    "    \n",
    "    def __init__(self, dataset, tag_preprocess=lambda x: x):\n",
    "        self.dataset = dataset\n",
    "        agg_func = lambda s: [\n",
    "            (w, p, tag_preprocess(t)) \n",
    "            for w, p, t in zip(s[\"word\"].values.tolist(), s['pos'].values.tolist(),\n",
    "                             s[\"tag\"].values.tolist())\n",
    "        ]\n",
    "        grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
    "        self.sentences = [s for s in grouped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a list of sentences from the dataset and we replace the BIO tag format for a regular label type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Thousands', 'NNS', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('demonstrators', 'NNS', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('marched', 'VBN', 'O'),\n",
       "  ('through', 'IN', 'O'),\n",
       "  ('London', 'NNP', 'geo'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('protest', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('war', 'NN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('Iraq', 'NNP', 'geo'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('demand', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('withdrawal', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('British', 'JJ', 'gpe'),\n",
       "  ('troops', 'NNS', 'O'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('that', 'DT', 'O'),\n",
       "  ('country', 'NN', 'O'),\n",
       "  ('.', '.', 'O')]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_bio = lambda x: x.replace('I-', '').replace('B-', '')\n",
    "\n",
    "instances = SentenceFactory(dataset, tag_preprocess=remove_bio).sentences\n",
    "\n",
    "instances[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length = dataset.groupby('sentence_idx').word.count().max()\n",
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 30175\n"
     ]
    }
   ],
   "source": [
    "unique_words = dataset.word.unique()\n",
    "unique_words = numpy.append(unique_words, \"ENDPAD\")\n",
    "print('Vocabulary size {}'.format(unique_words.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'geo' 'gpe' 'per' 'org' 'tim' 'art' 'nat' 'eve' 'prev-prev-lemma']\n",
      "Unique labels 10\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.tag.fillna('O').apply(remove_bio).unique()\n",
    "print(labels)\n",
    "print('Unique labels {}'.format(labels.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input sequences\n",
    "\n",
    "To train more effectively the network, we pad all sequences to have the same lenght. In this case, we choose to use the lenght of the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx = {w: i for i, w in enumerate(unique_words)}\n",
    "labels2idx = {t: i for i, t in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_matrix = [[word2idx[w[0]] for w in s] for s in instances]\n",
    "x_matrix = pad_sequences(maxlen=max_sentence_length, sequences=x_matrix,\n",
    "                         padding=\"post\", value=unique_words.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     1,     2, ..., 30174, 30174, 30174],\n",
       "       [   22,     1,    23, ..., 30174, 30174, 30174],\n",
       "       [   42,     4,    18, ..., 30174, 30174, 30174],\n",
       "       ..., \n",
       "       [   61,   921,   151, ..., 30174, 30174, 30174],\n",
       "       [  531,   330,     3, ..., 30174, 30174, 30174],\n",
       "       [18519, 30174, 30174, ..., 30174, 30174, 30174]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [[labels2idx[w[2]] for w in s] for s in instances]\n",
    "y = pad_sequences(maxlen=140, sequences=y, padding=\"post\", value=labels2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes=labels.shape[0]) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_matrix, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Building the model\n",
    "\n",
    "We build a model with an object oriented interface so we can add and remove layers in sub-classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiLSTM(object):\n",
    "    def __init__(self, vocabulary_size, max_sentence_length, labels,\n",
    "                 embedding_size=50):\n",
    "        self.model = None\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.labels = labels\n",
    "        self.n_labels = labels.shape[0]\n",
    "        \n",
    "    def add_embedding_layer(self, layers):\n",
    "        layers = Embedding(\n",
    "            input_dim=self.vocabulary_size,\n",
    "            output_dim=self.max_sentence_length,\n",
    "            input_length=self.max_sentence_length)(layers)\n",
    "        return Dropout(0.1)(layers)\n",
    "    \n",
    "    def add_recurrent_layer(self, layers):\n",
    "        return Bidirectional(\n",
    "            LSTM(units=100, return_sequences=True,\n",
    "                 recurrent_dropout=0.1))(layers)\n",
    "    \n",
    "    def add_output_layer(self, layers):\n",
    "        return TimeDistributed(\n",
    "            Dense(self.n_labels, activation=\"softmax\"))(layers)\n",
    "    \n",
    "    def build(self):\n",
    "        input = Input(shape=(self.max_sentence_length,))\n",
    "        layers = self.add_embedding_layer(input)\n",
    "        layers = self.add_recurrent_layer(layers)\n",
    "        layers = self.add_output_layer(layers)        \n",
    "        \n",
    "        self.model = Model(input, layers)\n",
    "        self.model.compile(\n",
    "            optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs, batch_size=32, validation_split=0.2):\n",
    "        if self.model is None:\n",
    "            self.build()\n",
    "        return self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                              validation_split=validation_split, verbose=1)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return numpy.argmax(self.model.predict(X_test), axis=-1)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        predictions = numpy.argmax(self.model.predict(X_test), axis=-1).flatten()\n",
    "        true_labels = numpy.argmax(y_test, axis=-1).flatten()\n",
    "        print(metrics.classification_report(true_labels, predictions,\n",
    "                                            target_names=self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BiLSTM(vocabulary_size=unique_words.shape[0],\n",
    "               max_sentence_length=max_sentence_length,\n",
    "               labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23478 samples, validate on 5870 samples\n",
      "Epoch 1/10\n",
      "23478/23478 [==============================] - 386s 16ms/step - loss: 0.0970 - acc: 0.9791 - val_loss: 0.0278 - val_acc: 0.9915\n",
      "Epoch 2/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0214 - acc: 0.9935 - val_loss: 0.0222 - val_acc: 0.9932\n",
      "Epoch 3/10\n",
      "23478/23478 [==============================] - 361s 15ms/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0205 - val_acc: 0.9937\n",
      "Epoch 4/10\n",
      "23478/23478 [==============================] - 363s 15ms/step - loss: 0.0120 - acc: 0.9961 - val_loss: 0.0211 - val_acc: 0.9937\n",
      "Epoch 5/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.0220 - val_acc: 0.9938\n",
      "Epoch 6/10\n",
      "23478/23478 [==============================] - 359s 15ms/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0233 - val_acc: 0.9936\n",
      "Epoch 7/10\n",
      "23478/23478 [==============================] - 359s 15ms/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0247 - val_acc: 0.9935\n",
      "Epoch 8/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0265 - val_acc: 0.9934\n",
      "Epoch 9/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9933\n",
      "Epoch 10/10\n",
      "23478/23478 [==============================] - 358s 15ms/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0292 - val_acc: 0.9932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff66a95d4a8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 100\n",
    "model.fit(X_train, numpy.array(y_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.save('model_10ep.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, we can load a previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.load_weights('../model_10ep.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "              O       1.00      1.00      1.00     13571\n",
      "            geo       0.95      0.90      0.93        91\n",
      "            gpe       0.97      0.98      0.98        60\n",
      "            per       0.93      0.95      0.94        91\n",
      "            org       0.95      0.90      0.92       101\n",
      "            tim       0.95      0.91      0.93        82\n",
      "            art       0.00      0.00      0.00         1\n",
      "            nat       1.00      0.33      0.50         3\n",
      "\n",
      "    avg / total       1.00      1.00      1.00     14000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milagro/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "size = 100\n",
    "model.evaluate(X_test[:size], y_test[:size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            : (True) : Pred\n",
      "The             : O      : O     \n",
      "United          : geo    : geo   \n",
      "States          : geo    : geo   \n",
      "and             : O      : O     \n",
      "other           : O      : O     \n",
      "Western         : O      : O     \n",
      "nations         : O      : O     \n",
      "are             : O      : O     \n",
      "trying          : O      : O     \n",
      "to              : O      : O     \n",
      "persuade        : O      : O     \n",
      "the             : O      : O     \n",
      "U.N.            : org    : org   \n",
      "Security        : org    : org   \n",
      "Council         : org    : org   \n",
      "to              : O      : O     \n",
      "impose          : O      : O     \n",
      "sanctions       : O      : O     \n",
      "on              : O      : O     \n",
      "Iran            : geo    : geo   \n",
      "because         : O      : O     \n",
      "of              : O      : O     \n",
      "its             : O      : O     \n",
      "nuclear         : O      : O     \n",
      "program         : O      : O     \n",
      ".               : O      : O     \n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "p = model.predict(numpy.array([X_test[i]]))\n",
    "print(\"{:15} : ({:4}) : {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "for w, true, pred in zip(X_test[i], y_test[i], p[i]):\n",
    "    if w == len(unique_words) - 1:\n",
    "        break\n",
    "    print(\"{:15} : {:6} : {:6}\".format(unique_words[w], labels[numpy.argmax(true)], labels[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Add an attention mechanism\n",
    "\n",
    "We implement the first solution given by the slides, calculating a single score per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Lambda, Permute, RepeatVector, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttBiLSTM(BiLSTM):\n",
    "    \n",
    "    def add_attention_block(self, layers):\n",
    "        \"\"\"Apply an attention block to a partial model layers.\"\"\"\n",
    "        feature_vector_size = K.int_shape(layers)[-1]\n",
    "        att_layer = Dense(feature_vector_size, activation='softmax',\n",
    "            name='attention_matrix_score')(layers)\n",
    "        # Calculate a single score for each timestep\n",
    "        att_layer = Lambda(lambda x: K.mean(x, axis=2),\n",
    "                           name='attention_vector_score')(att_layer)\n",
    "        # Reshape to obtain the same shape as input\n",
    "        att_layer = Permute((2, 1))(\n",
    "            RepeatVector(feature_vector_size)(att_layer))\n",
    "        layers = merge([att_layer, layers],  mode='mul')\n",
    "        return layers \n",
    "    \n",
    "    def add_embedding_layer(self, layers):\n",
    "        layers = super(AttBiLSTM, self).add_embedding_layer(layers)        \n",
    "        return self.add_attention_block(layers)\n",
    "    \n",
    "    def attention_predict(self, input_sequences):\n",
    "        \"\"\"Classifies the input sequences and returns the attention score.\n",
    "\n",
    "        Args:\n",
    "            model: a Keras model\n",
    "            input_: a list of array representation of sentences.\n",
    "\n",
    "        Returns:\n",
    "            A tuple where the first element is the attention scores for each\n",
    "            sentence, and the second is the model predictions.\n",
    "        \"\"\"\n",
    "        layer = self.model.get_layer('attention_vector_score')\n",
    "        attention_model = Model(\n",
    "            inputs=self.model.input, outputs=[layer.output, self.model.output])\n",
    "        # The attention output is (batch_size, timesteps, features)\n",
    "        return attention_model.predict(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = AttBiLSTM(vocabulary_size=unique_words.shape[0],\n",
    "               max_sentence_length=max_sentence_length,\n",
    "               labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23478 samples, validate on 5870 samples\n",
      "Epoch 1/2\n",
      "23478/23478 [==============================] - 371s 16ms/step - loss: 0.1275 - acc: 0.9765 - val_loss: 0.0298 - val_acc: 0.9911\n",
      "Epoch 2/2\n",
      " 4896/23478 [=====>........................] - ETA: 4:38 - loss: 0.0272 - acc: 0.9916"
     ]
    }
   ],
   "source": [
    "size = 100\n",
    "model.fit(X_train, numpy.array(y_train), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.save('model_10ep_att2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or we can load it from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milagro/miniconda2/envs/am_env/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/milagro/miniconda2/envs/am_env/lib/python3.5/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.model.load_weights('model_10ep_att.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attention, predictions = model.attention_predict(numpy.array(X_test[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 140)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second attention model\n",
    "\n",
    "We implement the Philippe Remy model, where we calculate the attention scores weighting all the tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttBiLSTM2(BiLSTM):\n",
    "    \n",
    "    def add_attention_block(self, layers):\n",
    "        \"\"\"Apply an attention block to a partial model layers.\"\"\"\n",
    "        timesteps = K.int_shape(layers)[-2]\n",
    "        att_layer = Permute((2, 1))(att_layer)\n",
    "        att_layer = TimeDistributed(\n",
    "            Dense(timesteps, activation=None),\n",
    "            name='attention_matrix_score')(att_layer)\n",
    "        # Calculate a single score for each timestep\n",
    "        att_layer = Lambda(lambda x: K.mean(x, axis=2),\n",
    "                           name='attention_vector_score')(att_layer)\n",
    "        # Reshape to obtain the same shape as input\n",
    "        att_layer = Permute((2, 1))(\n",
    "            RepeatVector(feature_vector_size)(att_layer))\n",
    "        layers = merge([att_layer, layers],  mode='mul')\n",
    "        return layers \n",
    "    \n",
    "    def add_embedding_layer(self, layers):\n",
    "        layers = super(AttBiLSTM, self).add_embedding_layer(layers)        \n",
    "        return self.add_attention_block(layers)\n",
    "    \n",
    "    def attention_predict(self, input_sequences):\n",
    "        \"\"\"Classifies the input sequences and returns the attention score.\n",
    "\n",
    "        Args:\n",
    "            model: a Keras model\n",
    "            input_: a list of array representation of sentences.\n",
    "\n",
    "        Returns:\n",
    "            A tuple where the first element is the attention scores for each\n",
    "            sentence, and the second is the model predictions.\n",
    "        \"\"\"\n",
    "        layer = self.model.get_layer('attention_vector_score')\n",
    "        attention_model = Model(\n",
    "            inputs=self.model.input, outputs=[layer.output, self.model.output])\n",
    "        # The attention output is (batch_size, timesteps, features)\n",
    "        return attention_model.predict(input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 100\n",
    "model.fit(X_train, numpy.array(y_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.save('model_10ep_att3.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or we can load it from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milagro/miniconda2/envs/am_env/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/milagro/miniconda2/envs/am_env/lib/python3.5/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.model.load_weights('model_10ep_att3.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Visualize the attention\n",
    "\n",
    "First, we align the attention and labels output from the network, and remove all the padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "# This could be done in a much more compact code, but I hope this is more\n",
    "# understandable\n",
    "for sentence_idx, (word_idxs, sentence_a_scores, sentence_labels) in enumerate(\n",
    "        zip(X_test[0:2], attention, numpy.argmax(predictions, axis=-1))):\n",
    "    for word_idx, a_score, label_idx in zip(word_idxs, sentence_a_scores, sentence_labels):\n",
    "        word = unique_words[word_idx]\n",
    "        if word == 'ENDPAD':\n",
    "            break\n",
    "        label = labels[label_idx]\n",
    "        result.append((word, a_score, sentence_idx, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 1.5802292, 0, 'O'),\n",
       " ('United', -4.1597695, 0, 'org'),\n",
       " ('States', -4.1535788, 0, 'geo'),\n",
       " ('and', -1.7216194, 0, 'O'),\n",
       " ('other', 2.8763144, 0, 'O')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a stand alone service, we first must store the results in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas.DataFrame(result, columns=['token', 'attention', 'sentence', 'label']).to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the file, you need to run a local http server to see the result. Run in the console from the repository directory:\n",
    "\n",
    "$ python -m http.server\n",
    "\n",
    "Then, open your browser in localhost:8000, and you should see the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualizing attention in notebook\n",
    "\n",
    "Another option is to import d3 directly into the notebook, but it is less robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from string import Template\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"js/d3.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<script src=\"js/d3.min.js\"></script>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"js/textChart.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<script src=\"js/textChart.js\"></script>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "if (d3 === undefined) {\n",
       "    alert('No d3 library');\n",
       "}\n",
       "if (TextChart === undefined) {\n",
       "    alert('No Chart library');\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"<script>\n",
    "if (d3 === undefined) {\n",
    "    alert('No d3 library');\n",
    "}\n",
    "if (TextChart === undefined) {\n",
    "    alert('No Chart library');\n",
    "}\n",
    "</script>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data = pandas.DataFrame(\n",
    "    result, columns=['token', 'attention', 'sentence', 'label']).to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id='text-container'></div>\n",
       "    <script>\n",
       "var nouns = [{\"token\":\"The\",\"attention\":1.5802291632,\"sentence\":0,\"label\":\"O\"},{\"token\":\"United\",\"attention\":-4.1597695351,\"sentence\":0,\"label\":\"org\"},{\"token\":\"States\",\"attention\":-4.1535787582,\"sentence\":0,\"label\":\"geo\"},{\"token\":\"and\",\"attention\":-1.7216193676,\"sentence\":0,\"label\":\"O\"},{\"token\":\"other\",\"attention\":2.8763144016,\"sentence\":0,\"label\":\"O\"},{\"token\":\"Western\",\"attention\":-1.238699317,\"sentence\":0,\"label\":\"org\"},{\"token\":\"nations\",\"attention\":2.548170805,\"sentence\":0,\"label\":\"O\"},{\"token\":\"are\",\"attention\":-2.3669812679,\"sentence\":0,\"label\":\"O\"},{\"token\":\"trying\",\"attention\":3.1176631451,\"sentence\":0,\"label\":\"O\"},{\"token\":\"to\",\"attention\":-1.7358671427,\"sentence\":0,\"label\":\"O\"},{\"token\":\"persuade\",\"attention\":-3.1014063358,\"sentence\":0,\"label\":\"O\"},{\"token\":\"the\",\"attention\":2.1367657185,\"sentence\":0,\"label\":\"O\"},{\"token\":\"U.N.\",\"attention\":-2.5363948345,\"sentence\":0,\"label\":\"org\"},{\"token\":\"Security\",\"attention\":-0.9154787064,\"sentence\":0,\"label\":\"org\"},{\"token\":\"Council\",\"attention\":-2.643882513,\"sentence\":0,\"label\":\"org\"},{\"token\":\"to\",\"attention\":-1.7358671427,\"sentence\":0,\"label\":\"O\"},{\"token\":\"impose\",\"attention\":-4.440735817,\"sentence\":0,\"label\":\"O\"},{\"token\":\"sanctions\",\"attention\":2.2970554829,\"sentence\":0,\"label\":\"O\"},{\"token\":\"on\",\"attention\":-1.965816617,\"sentence\":0,\"label\":\"O\"},{\"token\":\"Iran\",\"attention\":-6.0267572403,\"sentence\":0,\"label\":\"geo\"},{\"token\":\"because\",\"attention\":-2.9336936474,\"sentence\":0,\"label\":\"O\"},{\"token\":\"of\",\"attention\":-1.365200758,\"sentence\":0,\"label\":\"O\"},{\"token\":\"its\",\"attention\":2.5745546818,\"sentence\":0,\"label\":\"O\"},{\"token\":\"nuclear\",\"attention\":1.9978687763,\"sentence\":0,\"label\":\"O\"},{\"token\":\"program\",\"attention\":2.1973774433,\"sentence\":0,\"label\":\"O\"},{\"token\":\".\",\"attention\":-4.4272761345,\"sentence\":0,\"label\":\"O\"},{\"token\":\"In\",\"attention\":-4.7143349648,\"sentence\":1,\"label\":\"O\"},{\"token\":\"1860\",\"attention\":-2.223457098,\"sentence\":1,\"label\":\"tim\"},{\"token\":\",\",\"attention\":-1.987528801,\"sentence\":1,\"label\":\"O\"},{\"token\":\"Spain\",\"attention\":-3.0556020737,\"sentence\":1,\"label\":\"gpe\"},{\"token\":\"occupied\",\"attention\":2.3207712173,\"sentence\":1,\"label\":\"O\"},{\"token\":\"northern\",\"attention\":-4.66147995,\"sentence\":1,\"label\":\"O\"},{\"token\":\"Morocco\",\"attention\":-2.0442619324,\"sentence\":1,\"label\":\"geo\"},{\"token\":\"and\",\"attention\":-1.7216193676,\"sentence\":1,\"label\":\"O\"},{\"token\":\"ushered\",\"attention\":-1.1895250082,\"sentence\":1,\"label\":\"O\"},{\"token\":\"in\",\"attention\":-3.0508410931,\"sentence\":1,\"label\":\"O\"},{\"token\":\"a\",\"attention\":2.6206879616,\"sentence\":1,\"label\":\"O\"},{\"token\":\"half\",\"attention\":-1.558541894,\"sentence\":1,\"label\":\"O\"},{\"token\":\"century\",\"attention\":-1.7444748878,\"sentence\":1,\"label\":\"O\"},{\"token\":\"of\",\"attention\":-1.365200758,\"sentence\":1,\"label\":\"O\"},{\"token\":\"trade\",\"attention\":3.7024478912,\"sentence\":1,\"label\":\"O\"},{\"token\":\"rivalry\",\"attention\":-3.3674602509,\"sentence\":1,\"label\":\"O\"},{\"token\":\"among\",\"attention\":-1.7711805105,\"sentence\":1,\"label\":\"O\"},{\"token\":\"European\",\"attention\":-1.9352177382,\"sentence\":1,\"label\":\"O\"},{\"token\":\"powers\",\"attention\":-4.1713676453,\"sentence\":1,\"label\":\"O\"},{\"token\":\"that\",\"attention\":1.7345654964,\"sentence\":1,\"label\":\"O\"},{\"token\":\"saw\",\"attention\":-3.8900051117,\"sentence\":1,\"label\":\"O\"},{\"token\":\"Morocco\",\"attention\":-2.0442619324,\"sentence\":1,\"label\":\"geo\"},{\"token\":\"'s\",\"attention\":-1.7969111204,\"sentence\":1,\"label\":\"O\"},{\"token\":\"sovereignty\",\"attention\":-2.1672837734,\"sentence\":1,\"label\":\"O\"},{\"token\":\"steadily\",\"attention\":-3.7196733952,\"sentence\":1,\"label\":\"O\"},{\"token\":\"erode\",\"attention\":-2.0250837803,\"sentence\":1,\"label\":\"O\"},{\"token\":\";\",\"attention\":-2.7436685562,\"sentence\":1,\"label\":\"O\"},{\"token\":\"in\",\"attention\":-3.0508410931,\"sentence\":1,\"label\":\"O\"},{\"token\":\"1912\",\"attention\":-1.7900933027,\"sentence\":1,\"label\":\"tim\"},{\"token\":\",\",\"attention\":-1.987528801,\"sentence\":1,\"label\":\"O\"},{\"token\":\"the\",\"attention\":2.1367657185,\"sentence\":1,\"label\":\"O\"},{\"token\":\"French\",\"attention\":-3.904999733,\"sentence\":1,\"label\":\"gpe\"},{\"token\":\"imposed\",\"attention\":2.2046852112,\"sentence\":1,\"label\":\"O\"},{\"token\":\"a\",\"attention\":2.6206879616,\"sentence\":1,\"label\":\"O\"},{\"token\":\"protectorate\",\"attention\":4.3993721008,\"sentence\":1,\"label\":\"O\"},{\"token\":\"over\",\"attention\":1.4095736742,\"sentence\":1,\"label\":\"O\"},{\"token\":\"the\",\"attention\":2.1367657185,\"sentence\":1,\"label\":\"O\"},{\"token\":\"country\",\"attention\":3.3859519958,\"sentence\":1,\"label\":\"O\"},{\"token\":\".\",\"attention\":-4.4272761345,\"sentence\":1,\"label\":\"O\"}];  // We are heavily using the similarties\n",
       "                         // between js and json syntax.\n",
       "opts = {\n",
       "  lineHeight: 16,\n",
       "  width: 900,\n",
       "  height: 600,\n",
       "  linePadding: 10\n",
       "}\n",
       "chart = new TextChart(nouns, opts);\n",
       "chart.draw(\"text-container\");\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_text_template = Template('''\n",
    "var nouns = $json_data;  // We are heavily using the similarties\n",
    "                         // between js and json syntax.\n",
    "opts = {\n",
    "  lineHeight: 16,\n",
    "  width: 900,\n",
    "  height: 600,\n",
    "  linePadding: 10\n",
    "}\n",
    "chart = new TextChart(nouns, opts);\n",
    "chart.draw(\"text-container\");\n",
    "''')\n",
    "\n",
    "html_template = Template('''\n",
    "    <div id='text-container'></div>\n",
    "    <script>$js_text</script>\n",
    "''')\n",
    "\n",
    "js_text = js_text_template.substitute({\n",
    "    'json_data': json_data\n",
    "})\n",
    "\n",
    "HTML(html_template.substitute({'js_text': js_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:am_env]",
   "language": "python",
   "name": "conda-env-am_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
