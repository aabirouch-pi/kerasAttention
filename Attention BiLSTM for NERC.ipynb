{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Attention mechanisms to an LSTM network \n",
    "\n",
    "We will be working with the CONLL 2003 dataset, annotated for the task of Named Entity Recognition.\n",
    "\n",
    "The objetive of this notebook is to build a prototype LSTM for sequence labeling, and apply a very simple attention mechanisms before the recurrent layer. The base model is inpired in [this work](https://www.kaggle.com/gagandeep16/ner-using-bidirectional-lstm), by GaganBhatia. Most of the explanations of the code is in the accompaning slides.\n",
    "\n",
    "Once the model is trained, we show the attention score for each word.\n",
    "\n",
    "You can find two sample datasets directly hosted at UNC, [one](https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.csv) used by the original Kaggle notebook (150M) and a [smaller one](https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.sample.csv) just to play with (14M). If you are running in colab, just run the next cell with the corresponding URL to donwload the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /home/milagro/miniconda2/envs/am_env/lib/libcrypto.so.1.0.0: no version information available (required by wget)\n",
      "wget: /home/milagro/miniconda2/envs/am_env/lib/libssl.so.1.0.0: no version information available (required by wget)\n",
      "wget: /home/milagro/miniconda2/envs/am_env/lib/libssl.so.1.0.0: no version information available (required by wget)\n",
      "--2018-06-25 16:28:15--  https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.sample.csv\n",
      "Resolving cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)... 200.16.17.55\n",
      "Connecting to cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)|200.16.17.55|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14760771 (14M) [text/csv]\n",
      "Saving to: ‘data.sample.csv’\n",
      "\n",
      "data.sample.csv     100%[===================>]  14,08M  1,13MB/s    in 13s     \n",
      "\n",
      "2018-06-25 16:28:28 (1,12 MB/s) - ‘data.sample.csv’ saved [14760771/14760771]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O data.sample.csv -nc https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset and extracting sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dataset = pandas.read_csv(\"../ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False,\n",
    "                          usecols=['sentence_idx', 'word', 'pos', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050796, 4)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VBP</td>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VBN</td>\n",
       "      <td>1</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>through</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>London</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TO</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>protest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DT</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>war</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>demand</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DT</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "      <td>British</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>troops</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pos sentence_idx           word    tag\n",
       "0   NNS            1      Thousands      O\n",
       "1    IN            1             of      O\n",
       "2   NNS            1  demonstrators      O\n",
       "3   VBP            1           have      O\n",
       "4   VBN            1        marched      O\n",
       "5    IN            1        through      O\n",
       "6   NNP            1         London  B-geo\n",
       "7    TO            1             to      O\n",
       "8    VB            1        protest      O\n",
       "9    DT            1            the      O\n",
       "10   NN            1            war      O\n",
       "11   IN            1             in      O\n",
       "12  NNP            1           Iraq  B-geo\n",
       "13   CC            1            and      O\n",
       "14   VB            1         demand      O\n",
       "15   DT            1            the      O\n",
       "16   NN            1     withdrawal      O\n",
       "17   IN            1             of      O\n",
       "18   JJ            1        British  B-gpe\n",
       "19  NNS            1         troops      O"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentenceFactory(object):\n",
    "    \n",
    "    def __init__(self, dataset, tag_preprocess=lambda x: x):\n",
    "        self.dataset = dataset\n",
    "        agg_func = lambda s: [\n",
    "            (w, p, tag_preprocess(t)) \n",
    "            for w, p, t in zip(s[\"word\"].values.tolist(), s['pos'].values.tolist(),\n",
    "                             s[\"tag\"].values.tolist())\n",
    "        ]\n",
    "        grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
    "        self.sentences = [s for s in grouped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a list of sentences from the dataset and we replace the BIO tag format for a regular label type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Thousands', 'NNS', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('demonstrators', 'NNS', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('marched', 'VBN', 'O'),\n",
       "  ('through', 'IN', 'O'),\n",
       "  ('London', 'NNP', 'geo'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('protest', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('war', 'NN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('Iraq', 'NNP', 'geo'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('demand', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('withdrawal', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('British', 'JJ', 'gpe'),\n",
       "  ('troops', 'NNS', 'O'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('that', 'DT', 'O'),\n",
       "  ('country', 'NN', 'O'),\n",
       "  ('.', '.', 'O')]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_bio = lambda x: x.replace('I-', '').replace('B-', '')\n",
    "\n",
    "instances = SentenceFactory(dataset, tag_preprocess=remove_bio).sentences\n",
    "\n",
    "instances[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length = dataset.groupby('sentence_idx').word.count().max()\n",
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 30175\n"
     ]
    }
   ],
   "source": [
    "unique_words = dataset.word.unique()\n",
    "unique_words = numpy.append(unique_words, \"ENDPAD\")\n",
    "print('Vocabulary size {}'.format(unique_words.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'geo' 'gpe' 'per' 'org' 'tim' 'art' 'nat' 'eve' 'prev-prev-lemma']\n",
      "Unique labels 10\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.tag.fillna('O').apply(remove_bio).unique()\n",
    "print(labels)\n",
    "print('Unique labels {}'.format(labels.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input sequences\n",
    "\n",
    "To train more effectively the network, we pad all sequences to have the same lenght. In this case, we choose to use the lenght of the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx = {w: i for i, w in enumerate(unique_words)}\n",
    "labels2idx = {t: i for i, t in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_matrix = [[word2idx[w[0]] for w in s] for s in instances]\n",
    "x_matrix = pad_sequences(maxlen=max_sentence_length, sequences=x_matrix,\n",
    "                         padding=\"post\", value=unique_words.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     1,     2, ..., 30174, 30174, 30174],\n",
       "       [   22,     1,    23, ..., 30174, 30174, 30174],\n",
       "       [   42,     4,    18, ..., 30174, 30174, 30174],\n",
       "       ...,\n",
       "       [   61,   921,   151, ..., 30174, 30174, 30174],\n",
       "       [  531,   330,     3, ..., 30174, 30174, 30174],\n",
       "       [18519, 30174, 30174, ..., 30174, 30174, 30174]], dtype=int32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [[labels2idx[w[2]] for w in s] for s in instances]\n",
    "y = pad_sequences(maxlen=140, sequences=y, padding=\"post\", value=labels2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes=labels.shape[0]) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_matrix, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Building the model\n",
    "\n",
    "We build a model with an object oriented interface so we can add and remove layers in sub-classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiLSTM(object):\n",
    "    def __init__(self, vocabulary_size, max_sentence_length, labels,\n",
    "                 embedding_size=50):\n",
    "        self.model = None\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.labels = labels\n",
    "        self.n_labels = labels.shape[0]\n",
    "        \n",
    "    def add_embedding_layer(self, layers):\n",
    "        layers = Embedding(\n",
    "            input_dim=self.vocabulary_size,\n",
    "            output_dim=self.max_sentence_length,\n",
    "            input_length=self.max_sentence_length)(layers)\n",
    "        return Dropout(0.1)(layers)\n",
    "    \n",
    "    def add_recurrent_layer(self, layers):\n",
    "        return Bidirectional(\n",
    "            LSTM(units=100, return_sequences=True,\n",
    "                 recurrent_dropout=0.1))(layers)\n",
    "    \n",
    "    def add_output_layer(self, layers):\n",
    "        return TimeDistributed(\n",
    "            Dense(self.n_labels, activation=\"softmax\"))(layers)\n",
    "    \n",
    "    def build(self):\n",
    "        input = Input(shape=(self.max_sentence_length,))\n",
    "        layers = self.add_embedding_layer(input)\n",
    "        layers = self.add_recurrent_layer(layers)\n",
    "        layers = self.add_output_layer(layers)        \n",
    "        \n",
    "        self.model = Model(input, layers)\n",
    "        self.model.compile(\n",
    "            optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs, batch_size=32, validation_split=0.2):\n",
    "        if self.model is None:\n",
    "            self.build()\n",
    "        return self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                              validation_split=validation_split, verbose=1)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return numpy.argmax(self.model.predict(X_test), axis=-1)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        predictions = numpy.argmax(self.model.predict(X_test), axis=-1).flatten()\n",
    "        true_labels = numpy.argmax(y_test, axis=-1).flatten()\n",
    "        print(metrics.classification_report(true_labels, predictions,\n",
    "                                            target_names=self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BiLSTM(vocabulary_size=unique_words.shape[0],\n",
    "               max_sentence_length=max_sentence_length,\n",
    "               labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23478 samples, validate on 5870 samples\n",
      "Epoch 1/10\n",
      "23478/23478 [==============================] - 386s 16ms/step - loss: 0.0970 - acc: 0.9791 - val_loss: 0.0278 - val_acc: 0.9915\n",
      "Epoch 2/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0214 - acc: 0.9935 - val_loss: 0.0222 - val_acc: 0.9932\n",
      "Epoch 3/10\n",
      "23478/23478 [==============================] - 361s 15ms/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0205 - val_acc: 0.9937\n",
      "Epoch 4/10\n",
      "23478/23478 [==============================] - 363s 15ms/step - loss: 0.0120 - acc: 0.9961 - val_loss: 0.0211 - val_acc: 0.9937\n",
      "Epoch 5/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.0220 - val_acc: 0.9938\n",
      "Epoch 6/10\n",
      "23478/23478 [==============================] - 359s 15ms/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0233 - val_acc: 0.9936\n",
      "Epoch 7/10\n",
      "23478/23478 [==============================] - 359s 15ms/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0247 - val_acc: 0.9935\n",
      "Epoch 8/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0265 - val_acc: 0.9934\n",
      "Epoch 9/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9933\n",
      "Epoch 10/10\n",
      "23478/23478 [==============================] - 358s 15ms/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0292 - val_acc: 0.9932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff66a95d4a8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 100\n",
    "model.fit(X_train, numpy.array(y_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.save('model_10ep.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, we can load a previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.load_weights('model_10ep.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "              O       1.00      1.00      1.00    994779\n",
      "            geo       0.92      0.94      0.93      9071\n",
      "            gpe       0.98      0.95      0.96      3350\n",
      "            per       0.95      0.94      0.94      7014\n",
      "            org       0.93      0.85      0.89      7410\n",
      "            tim       0.93      0.96      0.94      5236\n",
      "            art       0.81      0.69      0.75       134\n",
      "            nat       0.78      0.73      0.75        55\n",
      "            eve       0.83      0.91      0.86       130\n",
      "prev-prev-lemma       0.00      0.00      0.00         1\n",
      "\n",
      "    avg / total       1.00      1.00      1.00   1027180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            : (True) : Pred\n",
      "The             : O      : O     \n",
      "United          : geo    : geo   \n",
      "States          : geo    : geo   \n",
      "and             : O      : O     \n",
      "other           : O      : O     \n",
      "Western         : O      : O     \n",
      "nations         : O      : O     \n",
      "are             : O      : O     \n",
      "trying          : O      : O     \n",
      "to              : O      : O     \n",
      "persuade        : O      : O     \n",
      "the             : O      : O     \n",
      "U.N.            : org    : org   \n",
      "Security        : org    : org   \n",
      "Council         : org    : org   \n",
      "to              : O      : O     \n",
      "impose          : O      : O     \n",
      "sanctions       : O      : O     \n",
      "on              : O      : O     \n",
      "Iran            : geo    : geo   \n",
      "because         : O      : O     \n",
      "of              : O      : O     \n",
      "its             : O      : O     \n",
      "nuclear         : O      : O     \n",
      "program         : O      : O     \n",
      ".               : O      : O     \n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "p = model.predict(numpy.array([X_test[i]]))\n",
    "print(\"{:15} : ({:4}) : {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "for w, true, pred in zip(X_test[i], y_test[i], p[i]):\n",
    "    if w == len(unique_words) - 1:\n",
    "        break\n",
    "    print(\"{:15} : {:6} : {:6}\".format(unique_words[w], labels[numpy.argmax(true)], labels[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Add an attention mechanism\n",
    "\n",
    "We implement the first solution given by the slides, calculating a single score per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Lambda, Permute, RepeatVector, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttBiLSTM(BiLSTM):\n",
    "    \n",
    "    def add_attention_block(self, layers):\n",
    "        \"\"\"Apply an attention block to a partial model layers.\"\"\"\n",
    "        feature_vector_size = K.int_shape(layers)[-1]\n",
    "        att_layer = Dense(feature_vector_size, activation='softmax', # activation=None,\n",
    "                          name='attention_matrix_score')(layers)\n",
    "        # Calculate a single score for each timestep\n",
    "        att_layer = Lambda(lambda x: K.mean(x, axis=1),\n",
    "                           name='attention_vector_score')(att_layer)\n",
    "        # Reshape to obtain the same shape as input\n",
    "        att_layer = Permute((2, 1))(\n",
    "            RepeatVector(feature_vector_size)(att_layer))\n",
    "        layers = merge([att_layer, layers],  mode='mul')\n",
    "        return layers \n",
    "    \n",
    "    def add_embedding_layer(self, layers):\n",
    "        layers = super(AttBiLSTM, self).add_embedding_layer(layers)        \n",
    "        return self.add_attention_block(layers)\n",
    "    \n",
    "    def attention_predict(self, input_sequences):\n",
    "        \"\"\"Classifies the input sequences and returns the attention score.\n",
    "\n",
    "        Args:\n",
    "            input_sequences: a list of array representation of sentences.\n",
    "\n",
    "        Returns:\n",
    "            A tuple where the first element is the attention scores for each\n",
    "            sentence, and the second is the model predictions.\n",
    "        \"\"\"\n",
    "        layer = self.model.get_layer('attention_vector_score')\n",
    "        attention_model = Model(\n",
    "            inputs=self.model.input, outputs=[layer.output, self.model.output])\n",
    "        # The attention output is (batch_size, timesteps, features)\n",
    "        return attention_model.predict(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = AttBiLSTM(vocabulary_size=unique_words.shape[0],\n",
    "                  max_sentence_length=max_sentence_length,\n",
    "                  labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23478 samples, validate on 5870 samples\n",
      "Epoch 1/10\n",
      "23478/23478 [==============================] - 369s 16ms/step - loss: 0.0777 - acc: 0.9757 - val_loss: 0.0632 - val_acc: 0.9788\n",
      "Epoch 2/10\n",
      "21056/23478 [=========================>....] - ETA: 35s - loss: 0.0531 - acc: 0.9821"
     ]
    }
   ],
   "source": [
    "size = 100\n",
    "model.fit(X_train, numpy.array(y_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.save('model_10ep_att_softmax.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or we can load it from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.model.load_weights('model_10ep_att_softmax.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "              O       1.00      1.00      1.00    994779\n",
      "            geo       0.83      0.83      0.83      9071\n",
      "            gpe       0.94      0.85      0.89      3350\n",
      "            per       0.89      0.77      0.83      7014\n",
      "            org       0.80      0.65      0.72      7410\n",
      "            tim       0.87      0.82      0.85      5236\n",
      "            art       0.00      0.00      0.00       134\n",
      "            nat       0.00      0.00      0.00        55\n",
      "            eve       0.10      0.02      0.03       130\n",
      "prev-prev-lemma       0.00      0.00      0.00         1\n",
      "\n",
      "    avg / total       0.99      0.99      0.99   1027180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attention, predictions = model.attention_predict(numpy.array(X_test[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01000638, 0.00979509, 0.03112146, 0.03014026, 0.01036052,\n",
       "        0.02828653, 0.04183299, 0.0112968 , 0.00954351, 0.00961488,\n",
       "        0.00934762, 0.00933148, 0.00927358, 0.00972058, 0.00904836,\n",
       "        0.00897365, 0.00904961, 0.0096976 , 0.00888072, 0.01068403,\n",
       "        0.00879953, 0.01101034, 0.00893785, 0.00906931, 0.00969644,\n",
       "        0.00946349, 0.0089477 , 0.0091248 , 0.00905322, 0.00946682,\n",
       "        0.00919382, 0.00931458, 0.00925128, 0.00932566, 0.0090926 ,\n",
       "        0.01046528, 0.00913227, 0.00911073, 0.00931868, 0.00912134,\n",
       "        0.00904317, 0.00907065, 0.0091409 , 0.00893204, 0.00884265,\n",
       "        0.00887148, 0.00859397, 0.00918771, 0.00873354, 0.01000163,\n",
       "        0.00946891, 0.00960682, 0.00965905, 0.01130545, 0.00991268,\n",
       "        0.00933771, 0.00947753, 0.00958044, 0.00955382, 0.00882774,\n",
       "        0.00910393, 0.00891715, 0.00858055, 0.00849193, 0.00861425,\n",
       "        0.00908855, 0.00831471, 0.00830295, 0.00807236, 0.00802214,\n",
       "        0.00849908, 0.00799347, 0.00770374, 0.00788084, 0.00736868,\n",
       "        0.00706101, 0.00703302, 0.00658179, 0.0075331 , 0.0068578 ,\n",
       "        0.00681064, 0.00657496, 0.00666358, 0.00641571, 0.00636563,\n",
       "        0.00619531, 0.00601912, 0.0055713 , 0.00638767, 0.0055353 ,\n",
       "        0.00476502, 0.00498875, 0.00509729, 0.00564357, 0.00396435,\n",
       "        0.00460591, 0.00316065, 0.00401445, 0.00413018, 0.00380291,\n",
       "        0.00419696, 0.00369194, 0.0017927 , 0.00345991, 0.00246855,\n",
       "        0.00171988, 0.00231818, 0.00192052, 0.00264268, 0.00279512,\n",
       "        0.00178414, 0.00154385, 0.00177953, 0.00159873, 0.00173916,\n",
       "        0.00189642, 0.00138327, 0.00164187, 0.00160449, 0.00150442,\n",
       "        0.00142627, 0.00132324, 0.00156091, 0.00156223, 0.0013128 ,\n",
       "        0.00130661, 0.00124201, 0.00122799, 0.00113387, 0.00115413,\n",
       "        0.00119436, 0.00114257, 0.00121691, 0.0012033 , 0.0013126 ,\n",
       "        0.00093732, 0.00085251, 0.00116093, 0.00243202, 0.0050324 ],\n",
       "       [0.00944305, 0.00928306, 0.03279332, 0.02892955, 0.00976236,\n",
       "        0.03647571, 0.05838438, 0.01069401, 0.00900572, 0.00910935,\n",
       "        0.00879155, 0.0087264 , 0.00874626, 0.00938612, 0.00871477,\n",
       "        0.00856524, 0.00857969, 0.00940799, 0.00846416, 0.01055487,\n",
       "        0.00833961, 0.01055037, 0.00849491, 0.00854103, 0.0093642 ,\n",
       "        0.00899507, 0.00856122, 0.00873556, 0.00855921, 0.00900576,\n",
       "        0.0086686 , 0.00878576, 0.00871706, 0.00866842, 0.00855596,\n",
       "        0.01019123, 0.00858106, 0.00878868, 0.00865244, 0.00864034,\n",
       "        0.00849464, 0.00871314, 0.00882427, 0.00844505, 0.00830167,\n",
       "        0.00836379, 0.00803221, 0.00866971, 0.00819661, 0.00951375,\n",
       "        0.00890427, 0.00905961, 0.00905313, 0.01093255, 0.00947771,\n",
       "        0.00878184, 0.0089359 , 0.00896059, 0.00879363, 0.00834896,\n",
       "        0.00848567, 0.00844507, 0.00792777, 0.00798098, 0.00825492,\n",
       "        0.00901965, 0.00779783, 0.00764927, 0.00756627, 0.00754062,\n",
       "        0.00816623, 0.00741314, 0.00738181, 0.00725716, 0.00686881,\n",
       "        0.0065777 , 0.00653356, 0.00616487, 0.00747152, 0.006733  ,\n",
       "        0.00645058, 0.00630656, 0.00655042, 0.00655295, 0.00661522,\n",
       "        0.00664189, 0.00563058, 0.00549067, 0.00745058, 0.00597917,\n",
       "        0.00480076, 0.00488083, 0.00542125, 0.00601018, 0.00458175,\n",
       "        0.00508332, 0.00330697, 0.00418582, 0.00420537, 0.00429086,\n",
       "        0.00467659, 0.00395825, 0.00197956, 0.00367021, 0.00277544,\n",
       "        0.00167325, 0.00259459, 0.00200802, 0.00264222, 0.00343129,\n",
       "        0.00203935, 0.00158301, 0.00196731, 0.00160841, 0.00199297,\n",
       "        0.00215545, 0.00150091, 0.00191069, 0.00171823, 0.00168129,\n",
       "        0.00154784, 0.00149521, 0.00175602, 0.00187692, 0.00150434,\n",
       "        0.00145496, 0.00137553, 0.00133906, 0.00117886, 0.00134166,\n",
       "        0.00134256, 0.00126246, 0.00139862, 0.00132042, 0.0016663 ,\n",
       "        0.00103448, 0.00096112, 0.00134589, 0.00260965, 0.00493243]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second attention model\n",
    "\n",
    "We implement the Philippe Remy model, where we calculate the attention scores weighting all the timesteps at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttBiLSTM2(BiLSTM):\n",
    "    \n",
    "    def add_attention_block(self, layers):\n",
    "        \"\"\"Apply an attention block to a partial model layers.\"\"\"\n",
    "        timesteps = K.int_shape(layers)[-2]\n",
    "        att_layer = Permute((2, 1))(layers)\n",
    "        att_layer = Dense(timesteps, activation='softmax', # activation=None,\n",
    "                          name='attention_matrix_score')(att_layer)\n",
    "        # Calculate a single score for each timestep\n",
    "        att_layer = Lambda(lambda x: K.mean(x, axis=1),\n",
    "                           name='attention_vector_score')(att_layer)\n",
    "        # Reshape to obtain the same shape as input\n",
    "        att_layer = Permute((2, 1))(RepeatVector(timesteps)(att_layer))\n",
    "        layers = merge([att_layer, layers],  mode='mul')\n",
    "        return layers \n",
    "    \n",
    "    def add_embedding_layer(self, layers):\n",
    "        layers = super(AttBiLSTM2, self).add_embedding_layer(layers)        \n",
    "        return self.add_attention_block(layers)\n",
    "    \n",
    "    def attention_predict(self, input_sequences):\n",
    "        \"\"\"Classifies the input sequences and returns the attention score.\n",
    "\n",
    "        Args:\n",
    "            input_sequences: a list of array representation of sentences.\n",
    "\n",
    "        Returns:\n",
    "            A tuple where the first element is the attention scores for each\n",
    "            sentence, and the second is the model predictions.\n",
    "        \"\"\"\n",
    "        layer = self.model.get_layer('attention_vector_score')\n",
    "        attention_model = Model(\n",
    "            inputs=self.model.input, outputs=[layer.output, self.model.output])\n",
    "        # The attention output is (batch_size, timesteps, features)\n",
    "        return attention_model.predict(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = AttBiLSTM2(vocabulary_size=unique_words.shape[0],\n",
    "                   max_sentence_length=max_sentence_length,\n",
    "                   labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 100\n",
    "model.fit(X_train, numpy.array(y_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.save('model_10ep_att2_softmax.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or we can load it from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.model.load_weights('model_10ep_att2_softmax.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally evaluate the model and get some sample attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "              O       1.00      1.00      1.00    994779\n",
      "            geo       0.82      0.87      0.85      9071\n",
      "            gpe       0.96      0.89      0.92      3350\n",
      "            per       0.91      0.79      0.85      7014\n",
      "            org       0.84      0.68      0.75      7410\n",
      "            tim       0.91      0.85      0.88      5236\n",
      "            art       0.00      0.00      0.00       134\n",
      "            nat       0.00      0.00      0.00        55\n",
      "            eve       0.50      0.01      0.02       130\n",
      "prev-prev-lemma       0.00      0.00      0.00         1\n",
      "\n",
      "    avg / total       0.99      0.99      0.99   1027180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attention, predictions = model.attention_predict(numpy.array(X_test[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01988118, 0.01532837, 0.01745828, 0.02029429, 0.01645563,\n",
       "        0.01321826, 0.01471941, 0.01522901, 0.017341  , 0.01651317,\n",
       "        0.01266951, 0.01955203, 0.0268642 , 0.01956076, 0.01492754,\n",
       "        0.01824682, 0.01587212, 0.02207735, 0.01965686, 0.02398645,\n",
       "        0.02009648, 0.01991243, 0.01306573, 0.01767619, 0.01840059,\n",
       "        0.01783916, 0.02105205, 0.01719425, 0.01869447, 0.01663652,\n",
       "        0.01668748, 0.01739269, 0.01590375, 0.01081016, 0.01316883,\n",
       "        0.00844534, 0.0154878 , 0.01242747, 0.01088647, 0.01504009,\n",
       "        0.01790265, 0.02157492, 0.02769303, 0.00967434, 0.011865  ,\n",
       "        0.00807473, 0.01186555, 0.00610112, 0.01052454, 0.00582738,\n",
       "        0.00867038, 0.00547996, 0.00650602, 0.00207297, 0.0052943 ,\n",
       "        0.00219738, 0.0062105 , 0.00153253, 0.00273718, 0.00149692,\n",
       "        0.00245146, 0.00100101, 0.00121036, 0.00101179, 0.00101948,\n",
       "        0.00054415, 0.00114498, 0.00053637, 0.00109128, 0.00049385,\n",
       "        0.00059793, 0.00049596, 0.00073868, 0.00080347, 0.000445  ,\n",
       "        0.00043578, 0.00044705, 0.00041112, 0.00068821, 0.00040069,\n",
       "        0.00038183, 0.00039304, 0.0004126 , 0.00040462, 0.00038275,\n",
       "        0.00137211, 0.00041668, 0.00044816, 0.00043692, 0.00106834,\n",
       "        0.00117526, 0.00042984, 0.00037639, 0.00072063, 0.00038386,\n",
       "        0.00053135, 0.00043893, 0.00047046, 0.00056449, 0.00045319,\n",
       "        0.00056518, 0.00034507, 0.00037791, 0.00037876, 0.00038437,\n",
       "        0.00037972, 0.00029901, 0.00037069, 0.00032069, 0.00038563,\n",
       "        0.00031943, 0.00027897, 0.00028846, 0.00032402, 0.00030151,\n",
       "        0.00033089, 0.00025923, 0.00027156, 0.00028089, 0.00030963,\n",
       "        0.00025979, 0.00026023, 0.00029444, 0.00024178, 0.00026513,\n",
       "        0.00026375, 0.00023659, 0.00025476, 0.00024248, 0.00026584,\n",
       "        0.0002319 , 0.00025246, 0.00026051, 0.00025887, 0.00029774,\n",
       "        0.00036176, 0.00038998, 0.00052439, 0.00069751, 0.11016682],\n",
       "       [0.0251227 , 0.02068112, 0.01432291, 0.02034094, 0.01619862,\n",
       "        0.01512827, 0.0137328 , 0.01619042, 0.01888556, 0.01742195,\n",
       "        0.01847615, 0.02001644, 0.01707379, 0.01464846, 0.03068022,\n",
       "        0.01949576, 0.01708172, 0.01948578, 0.01966095, 0.0223811 ,\n",
       "        0.01774869, 0.02261561, 0.01264406, 0.01706101, 0.01753413,\n",
       "        0.01454139, 0.02265716, 0.02135109, 0.01532891, 0.01459928,\n",
       "        0.01566947, 0.0160023 , 0.01182378, 0.01226498, 0.0121889 ,\n",
       "        0.00727814, 0.0111249 , 0.01395155, 0.01031283, 0.0139687 ,\n",
       "        0.01462398, 0.01854082, 0.02940474, 0.01036212, 0.01015034,\n",
       "        0.00783675, 0.00980619, 0.00405402, 0.0095777 , 0.00498572,\n",
       "        0.00848219, 0.00431534, 0.00608203, 0.00220896, 0.00385328,\n",
       "        0.00228866, 0.00749842, 0.00149711, 0.0027649 , 0.00163014,\n",
       "        0.0028915 , 0.00104482, 0.00124409, 0.00106446, 0.00100147,\n",
       "        0.00054337, 0.00115291, 0.00051624, 0.00106459, 0.0004334 ,\n",
       "        0.00050814, 0.00045581, 0.00072078, 0.00076112, 0.00044205,\n",
       "        0.00038921, 0.00039682, 0.00036304, 0.00074326, 0.00036656,\n",
       "        0.00030492, 0.00031958, 0.00036126, 0.00031295, 0.00032377,\n",
       "        0.00115576, 0.00030811, 0.00034063, 0.0003111 , 0.00098979,\n",
       "        0.00094533, 0.00041757, 0.00032655, 0.00057018, 0.00038379,\n",
       "        0.00056872, 0.00039042, 0.00039137, 0.00044667, 0.00041306,\n",
       "        0.00055453, 0.00030685, 0.00033131, 0.0003657 , 0.00036186,\n",
       "        0.00036001, 0.00026358, 0.00033489, 0.00031732, 0.00037283,\n",
       "        0.00031113, 0.00026245, 0.00027493, 0.00032365, 0.00029592,\n",
       "        0.00033456, 0.00023954, 0.00026164, 0.00027519, 0.00031302,\n",
       "        0.00024593, 0.00024946, 0.00028388, 0.00022858, 0.00024976,\n",
       "        0.00026077, 0.000224  , 0.00025778, 0.00023725, 0.00025499,\n",
       "        0.00022673, 0.00024844, 0.00025475, 0.00025514, 0.00028077,\n",
       "        0.00034761, 0.00034912, 0.00046748, 0.00077763, 0.12449782]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Visualize the attention\n",
    "\n",
    "First, we align the attention and labels output from the network, and remove all the padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "# This could be done in a much more compact code, but I hope this is more\n",
    "# understandable\n",
    "for sentence_idx, (word_idxs, sentence_a_scores, sentence_labels) in enumerate(\n",
    "        zip(X_test[0:2], attention, numpy.argmax(predictions, axis=-1))):\n",
    "    for word_idx, a_score, label_idx in zip(word_idxs, sentence_a_scores, sentence_labels):\n",
    "        word = unique_words[word_idx]\n",
    "        if word == 'ENDPAD':\n",
    "            break\n",
    "        label = labels[label_idx]\n",
    "        result.append((word, abs(a_score), sentence_idx, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Among', 0.01988118, 0, 'O'),\n",
       " ('those', 0.015328373, 0, 'O'),\n",
       " ('freed', 0.017458279, 0, 'O'),\n",
       " ('earlier', 0.020294288, 0, 'O'),\n",
       " ('this', 0.01645563, 0, 'O')]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a stand alone service, we first must store the results in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas.DataFrame(result, columns=['token', 'attention', 'sentence', 'label']).to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the file, you need to run a local http server to see the result. Run in the console from the repository directory:\n",
    "\n",
    "```$ python -m http.server```\n",
    "\n",
    "Then, open your browser in localhost:8000, and you should see the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualizing attention in notebook\n",
    "\n",
    "Another option is to import d3 directly into the notebook, but it is less robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from string import Template\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"js/d3.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<script src=\"js/d3.min.js\"></script>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"js/textChart.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<script src=\"js/textChart.js\"></script>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "if (d3 === undefined) {\n",
       "    alert('No d3 library');\n",
       "}\n",
       "if (TextChart === undefined) {\n",
       "    alert('No Chart library');\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"<script>\n",
    "if (d3 === undefined) {\n",
    "    alert('No d3 library');\n",
    "}\n",
    "if (TextChart === undefined) {\n",
    "    alert('No Chart library');\n",
    "}\n",
    "</script>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data = pandas.DataFrame(\n",
    "    result, columns=['token', 'attention', 'sentence', 'label']).to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id='text-container'></div>\n",
       "    <script>\n",
       "var nouns = [{\"token\":\"Among\",\"attention\":5.2628922462,\"sentence\":0,\"label\":\"O\"},{\"token\":\"those\",\"attention\":4.1495289803,\"sentence\":0,\"label\":\"O\"},{\"token\":\"freed\",\"attention\":4.9908065796,\"sentence\":0,\"label\":\"O\"},{\"token\":\"earlier\",\"attention\":4.1774878502,\"sentence\":0,\"label\":\"O\"},{\"token\":\"this\",\"attention\":4.4258780479,\"sentence\":0,\"label\":\"O\"},{\"token\":\"week\",\"attention\":5.8671946526,\"sentence\":0,\"label\":\"O\"},{\"token\":\"were\",\"attention\":5.5076117516,\"sentence\":0,\"label\":\"O\"},{\"token\":\"well-known\",\"attention\":5.5720772743,\"sentence\":0,\"label\":\"O\"},{\"token\":\"dissident\",\"attention\":5.5492062569,\"sentence\":0,\"label\":\"O\"},{\"token\":\"writer\",\"attention\":5.4597783089,\"sentence\":0,\"label\":\"O\"},{\"token\":\"and\",\"attention\":5.2157096863,\"sentence\":0,\"label\":\"O\"},{\"token\":\"poet\",\"attention\":5.5451540947,\"sentence\":0,\"label\":\"O\"},{\"token\":\"Raul\",\"attention\":5.2043976784,\"sentence\":0,\"label\":\"per\"},{\"token\":\"Rivero\",\"attention\":6.2675008774,\"sentence\":0,\"label\":\"per\"},{\"token\":\",\",\"attention\":4.9160928726,\"sentence\":0,\"label\":\"O\"},{\"token\":\"opposition\",\"attention\":5.3409500122,\"sentence\":0,\"label\":\"O\"},{\"token\":\"politician\",\"attention\":5.7473096848,\"sentence\":0,\"label\":\"O\"},{\"token\":\"Osvaldo\",\"attention\":5.0037856102,\"sentence\":0,\"label\":\"per\"},{\"token\":\"Alfonso\",\"attention\":5.350435257,\"sentence\":0,\"label\":\"per\"},{\"token\":\"Valdes\",\"attention\":5.6070671082,\"sentence\":0,\"label\":\"per\"},{\"token\":\"and\",\"attention\":4.9009518623,\"sentence\":0,\"label\":\"O\"},{\"token\":\"economist\",\"attention\":5.820857048,\"sentence\":0,\"label\":\"O\"},{\"token\":\"and\",\"attention\":5.7699661255,\"sentence\":0,\"label\":\"O\"},{\"token\":\"journalist\",\"attention\":5.0282597542,\"sentence\":0,\"label\":\"O\"},{\"token\":\"Oscar\",\"attention\":5.8413496017,\"sentence\":0,\"label\":\"per\"},{\"token\":\"Espinoso\",\"attention\":5.8255486488,\"sentence\":0,\"label\":\"per\"},{\"token\":\"Chepe\",\"attention\":6.2376098633,\"sentence\":0,\"label\":\"per\"},{\"token\":\".\",\"attention\":5.1921401024,\"sentence\":0,\"label\":\"O\"},{\"token\":\"Venezuela\",\"attention\":5.3091974258,\"sentence\":1,\"label\":\"geo\"},{\"token\":\"'s\",\"attention\":4.1772651672,\"sentence\":1,\"label\":\"O\"},{\"token\":\"state-owned\",\"attention\":5.0238361359,\"sentence\":1,\"label\":\"O\"},{\"token\":\"oil\",\"attention\":4.1964716911,\"sentence\":1,\"label\":\"O\"},{\"token\":\"company\",\"attention\":4.4589557648,\"sentence\":1,\"label\":\"O\"},{\"token\":\"says\",\"attention\":5.8908243179,\"sentence\":1,\"label\":\"O\"},{\"token\":\"it\",\"attention\":5.5454668999,\"sentence\":1,\"label\":\"O\"},{\"token\":\"is\",\"attention\":5.6192550659,\"sentence\":1,\"label\":\"O\"},{\"token\":\"beginning\",\"attention\":5.5931334496,\"sentence\":1,\"label\":\"O\"},{\"token\":\"to\",\"attention\":5.499358654,\"sentence\":1,\"label\":\"O\"},{\"token\":\"explore\",\"attention\":5.2553725243,\"sentence\":1,\"label\":\"O\"},{\"token\":\"for\",\"attention\":5.5866832733,\"sentence\":1,\"label\":\"O\"},{\"token\":\"oil\",\"attention\":5.2297945023,\"sentence\":1,\"label\":\"O\"},{\"token\":\"in\",\"attention\":6.3087663651,\"sentence\":1,\"label\":\"O\"},{\"token\":\"Cuban\",\"attention\":4.9592599869,\"sentence\":1,\"label\":\"gpe\"},{\"token\":\"waters\",\"attention\":5.3783545494,\"sentence\":1,\"label\":\"O\"},{\"token\":\"as\",\"attention\":5.7902240753,\"sentence\":1,\"label\":\"O\"},{\"token\":\"part\",\"attention\":5.0280995369,\"sentence\":1,\"label\":\"O\"},{\"token\":\"of\",\"attention\":5.3942842484,\"sentence\":1,\"label\":\"O\"},{\"token\":\"a\",\"attention\":5.6496601105,\"sentence\":1,\"label\":\"O\"},{\"token\":\"joint\",\"attention\":4.9294261932,\"sentence\":1,\"label\":\"O\"},{\"token\":\"venture\",\"attention\":5.8703203201,\"sentence\":1,\"label\":\"O\"},{\"token\":\"with\",\"attention\":5.816865921,\"sentence\":1,\"label\":\"O\"},{\"token\":\"the\",\"attention\":5.063416481,\"sentence\":1,\"label\":\"O\"},{\"token\":\"island\",\"attention\":5.873295784,\"sentence\":1,\"label\":\"O\"},{\"token\":\"'s\",\"attention\":5.8711295128,\"sentence\":1,\"label\":\"O\"},{\"token\":\"state-owned\",\"attention\":6.2806444168,\"sentence\":1,\"label\":\"O\"},{\"token\":\"Cubapetroleo\",\"attention\":5.2365736961,\"sentence\":1,\"label\":\"O\"},{\"token\":\".\",\"attention\":5.7639899254,\"sentence\":1,\"label\":\"O\"}];  // We are heavily using the similarties\n",
       "                         // between js and json syntax.\n",
       "opts = {\n",
       "  lineHeight: 16,\n",
       "  width: 900,\n",
       "  height: 600,\n",
       "  linePadding: 10\n",
       "}\n",
       "chart = new TextChart(nouns, opts);\n",
       "chart.draw(\"text-container\");\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_text_template = Template('''\n",
    "var nouns = $json_data;  // We are heavily using the similarties\n",
    "                         // between js and json syntax.\n",
    "opts = {\n",
    "  lineHeight: 16,\n",
    "  width: 900,\n",
    "  height: 600,\n",
    "  linePadding: 10\n",
    "}\n",
    "chart = new TextChart(nouns, opts);\n",
    "chart.draw(\"text-container\");\n",
    "''')\n",
    "\n",
    "html_template = Template('''\n",
    "    <div id='text-container'></div>\n",
    "    <script>$js_text</script>\n",
    "''')\n",
    "\n",
    "js_text = js_text_template.substitute({\n",
    "    'json_data': json_data\n",
    "})\n",
    "\n",
    "HTML(html_template.substitute({'js_text': js_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:am_env]",
   "language": "python",
   "name": "conda-env-am_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
