{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Attention mechanisms to an LSTM network \n",
    "\n",
    "We will be working with the CONLL 2003 dataset, annotated for the task of Named Entity Recognition.\n",
    "\n",
    "The objetive of this notebook is to build a prototype LSTM for sequence labeling, and apply a very simple attention mechanisms before the recurrent layer. The base model is inpired in [this work](https://www.kaggle.com/gagandeep16/ner-using-bidirectional-lstm), by GaganBhatia\n",
    "\n",
    "You can find two sample datasets directly hosted at UNC, [one](https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.csv) used by the original Kaggle notebook (150M) and a [smaller one](https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.sample.csv) just to play with (14M).\n",
    "\n",
    "\n",
    "Once the model is trained, we show the attention score for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset and extracting sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dataset = pandas.read_csv(\"../ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False,\n",
    "                          usecols=['sentence_idx', 'word', 'pos', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050796, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VBP</td>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VBN</td>\n",
       "      <td>1</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>through</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>London</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TO</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>protest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DT</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>war</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>demand</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DT</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "      <td>British</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>troops</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pos sentence_idx           word    tag\n",
       "0   NNS            1      Thousands      O\n",
       "1    IN            1             of      O\n",
       "2   NNS            1  demonstrators      O\n",
       "3   VBP            1           have      O\n",
       "4   VBN            1        marched      O\n",
       "5    IN            1        through      O\n",
       "6   NNP            1         London  B-geo\n",
       "7    TO            1             to      O\n",
       "8    VB            1        protest      O\n",
       "9    DT            1            the      O\n",
       "10   NN            1            war      O\n",
       "11   IN            1             in      O\n",
       "12  NNP            1           Iraq  B-geo\n",
       "13   CC            1            and      O\n",
       "14   VB            1         demand      O\n",
       "15   DT            1            the      O\n",
       "16   NN            1     withdrawal      O\n",
       "17   IN            1             of      O\n",
       "18   JJ            1        British  B-gpe\n",
       "19  NNS            1         troops      O"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceFactory(object):\n",
    "    \n",
    "    def __init__(self, dataset, tag_preprocess=lambda x: x):\n",
    "        self.dataset = dataset\n",
    "        agg_func = lambda s: [\n",
    "            (w, p, tag_preprocess(t)) \n",
    "            for w, p, t in zip(s[\"word\"].values.tolist(), s['pos'].values.tolist(),\n",
    "                             s[\"tag\"].values.tolist())\n",
    "        ]\n",
    "        grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
    "        self.sentences = [s for s in grouped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a list of sentences from the dataset and we replace the BIO tag format for a regular label type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Thousands', 'NNS', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('demonstrators', 'NNS', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('marched', 'VBN', 'O'),\n",
       "  ('through', 'IN', 'O'),\n",
       "  ('London', 'NNP', 'geo'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('protest', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('war', 'NN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('Iraq', 'NNP', 'geo'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('demand', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('withdrawal', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('British', 'JJ', 'gpe'),\n",
       "  ('troops', 'NNS', 'O'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('that', 'DT', 'O'),\n",
       "  ('country', 'NN', 'O'),\n",
       "  ('.', '.', 'O')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_bio = lambda x: x.replace('I-', '').replace('B-', '')\n",
    "\n",
    "instances = SentenceFactory(dataset, tag_preprocess=remove_bio).sentences\n",
    "\n",
    "instances[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length = dataset.groupby('sentence_idx').word.count().max()\n",
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 30175\n"
     ]
    }
   ],
   "source": [
    "unique_words = dataset.word.unique()\n",
    "unique_words = numpy.append(unique_words, \"ENDPAD\")\n",
    "print('Vocabulary size {}'.format(unique_words.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'geo' 'gpe' 'per' 'org' 'tim' 'art' 'nat' 'eve' 'prev-prev-lemma']\n",
      "Unique labels 10\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.tag.fillna('O').apply(remove_bio).unique()\n",
    "print(labels)\n",
    "print('Unique labels {}'.format(labels.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input sequences\n",
    "\n",
    "To train more effectively the network, we pad all sequences to have the same lenght. In this case, we choose to use the lenght of the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i for i, w in enumerate(unique_words)}\n",
    "labels2idx = {t: i for i, t in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_matrix = [[word2idx[w[0]] for w in s] for s in instances]\n",
    "x_matrix = pad_sequences(maxlen=max_sentence_length, sequences=x_matrix,\n",
    "                         padding=\"post\", value=unique_words.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     1,     2, ..., 30174, 30174, 30174],\n",
       "       [   22,     1,    23, ..., 30174, 30174, 30174],\n",
       "       [   42,     4,    18, ..., 30174, 30174, 30174],\n",
       "       ...,\n",
       "       [   61,   921,   151, ..., 30174, 30174, 30174],\n",
       "       [  531,   330,     3, ..., 30174, 30174, 30174],\n",
       "       [18519, 30174, 30174, ..., 30174, 30174, 30174]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[labels2idx[w[2]] for w in s] for s in instances]\n",
    "y = pad_sequences(maxlen=140, sequences=y, padding=\"post\", value=labels2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes=labels.shape[0]) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_matrix, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Building the model\n",
    "\n",
    "We build a model with an object oriented interface so we can add and remove layers in sub-classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(object):\n",
    "    def __init__(self, vocabulary_size, max_sentence_length, labels,\n",
    "                 embedding_size=50):\n",
    "        self.model = None\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.labels = labels\n",
    "        self.n_labels = labels.shape[0]\n",
    "        \n",
    "    def add_embedding_layer(self, layers):\n",
    "        layers = Embedding(\n",
    "            input_dim=self.vocabulary_size,\n",
    "            output_dim=self.max_sentence_length,\n",
    "            input_length=self.max_sentence_length)(layers)\n",
    "        return Dropout(0.1)(layers)\n",
    "    \n",
    "    def add_recurrent_layer(self, layers):\n",
    "        return Bidirectional(\n",
    "            LSTM(units=100, return_sequences=True,\n",
    "                 recurrent_dropout=0.1))(layers)\n",
    "    \n",
    "    def add_output_layer(self, layers):\n",
    "        return TimeDistributed(\n",
    "            Dense(self.n_labels, activation=\"softmax\"))(layers)\n",
    "    \n",
    "    def build(self):\n",
    "        input = Input(shape=(self.max_sentence_length,))\n",
    "        layers = self.add_embedding_layer(input)\n",
    "        layers = self.add_recurrent_layer(layers)\n",
    "        layers = self.add_output_layer(layers)        \n",
    "        \n",
    "        self.model = Model(input, layers)\n",
    "        self.model.compile(\n",
    "            optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs, batch_size=32, validation_split=0.2):\n",
    "        if self.model is None:\n",
    "            self.build()\n",
    "        return self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                              validation_split=validation_split, verbose=1)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return numpy.argmax(self.model.predict(X_test), axis=-1)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        predictions = numpy.argmax(self.model.predict(X_test), axis=-1).flatten()\n",
    "        true_labels = numpy.argmax(y_test, axis=-1).flatten()\n",
    "        print(metrics.classification_report(true_labels, predictions,\n",
    "                                            target_names=self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(vocabulary_size=unique_words.shape[0],\n",
    "               max_sentence_length=max_sentence_length,\n",
    "               labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23478 samples, validate on 5870 samples\n",
      "Epoch 1/10\n",
      "23478/23478 [==============================] - 386s 16ms/step - loss: 0.0970 - acc: 0.9791 - val_loss: 0.0278 - val_acc: 0.9915\n",
      "Epoch 2/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0214 - acc: 0.9935 - val_loss: 0.0222 - val_acc: 0.9932\n",
      "Epoch 3/10\n",
      "23478/23478 [==============================] - 361s 15ms/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0205 - val_acc: 0.9937\n",
      "Epoch 4/10\n",
      "23478/23478 [==============================] - 363s 15ms/step - loss: 0.0120 - acc: 0.9961 - val_loss: 0.0211 - val_acc: 0.9937\n",
      "Epoch 5/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.0220 - val_acc: 0.9938\n",
      "Epoch 6/10\n",
      "23478/23478 [==============================] - 359s 15ms/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0233 - val_acc: 0.9936\n",
      "Epoch 7/10\n",
      "23478/23478 [==============================] - 359s 15ms/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0247 - val_acc: 0.9935\n",
      "Epoch 8/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0265 - val_acc: 0.9934\n",
      "Epoch 9/10\n",
      "23478/23478 [==============================] - 360s 15ms/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9933\n",
      "Epoch 10/10\n",
      "23478/23478 [==============================] - 358s 15ms/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0292 - val_acc: 0.9932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff66a95d4a8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 100\n",
    "model.fit(X_train, numpy.array(y_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save('model_10ep.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, we can load a previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.load_weights('model_10ep.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "              O       1.00      1.00      1.00    995317\n",
      "            geo       0.84      0.85      0.85      9014\n",
      "            gpe       0.95      0.93      0.94      3177\n",
      "            per       0.88      0.85      0.86      6882\n",
      "            org       0.81      0.71      0.76      7301\n",
      "            tim       0.85      0.89      0.87      5196\n",
      "            art       0.39      0.18      0.25       144\n",
      "            nat       0.51      0.40      0.45        45\n",
      "            eve       0.42      0.47      0.45       104\n",
      "\n",
      "    avg / total       0.99      0.99      0.99   1027180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 9, does not match size of target_names, 10\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            : (True) : Pred\n",
      "Health          : O      : O     \n",
      "officials       : O      : O     \n",
      "in              : O      : O     \n",
      "Indonesia       : geo    : geo   \n",
      "say             : O      : O     \n",
      "local           : O      : O     \n",
      "tests           : O      : O     \n",
      "show            : O      : O     \n",
      "an              : O      : O     \n",
      "Indonesian      : gpe    : gpe   \n",
      "man             : O      : O     \n",
      "who             : O      : O     \n",
      "died            : O      : O     \n",
      "last            : O      : O     \n",
      "week            : O      : O     \n",
      "was             : O      : O     \n",
      "infected        : O      : O     \n",
      "with            : O      : O     \n",
      "the             : O      : O     \n",
      "bird            : O      : O     \n",
      "flu             : O      : O     \n",
      "virus           : O      : O     \n",
      ".               : O      : O     \n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "p = model.predict(numpy.array([X_test[i]]))\n",
    "print(\"{:15} : ({:4}) : {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "for w, true, pred in zip(X_test[i], y_test[i], p[i]):\n",
    "    if w == len(unique_words) - 1:\n",
    "        break\n",
    "    print(\"{:15} : {:6} : {:6}\".format(unique_words[w], labels[numpy.argmax(true)], labels[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Add an attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Lambda, Permute, RepeatVector, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttBiLSTM(BiLSTM):\n",
    "    \n",
    "    def add_attention_block(self, layers):\n",
    "        \"\"\"Apply an attention block to a partial model layers.\"\"\"\n",
    "        feature_vector_size = K.int_shape(layers)[-1]\n",
    "        att_layer = Dense(feature_vector_size, activation=None,\n",
    "            name='attention_matrix_score')(layers)\n",
    "        # Calculate a single score for each timestep\n",
    "        att_layer = Lambda(lambda x: K.mean(x, axis=2),\n",
    "                           name='attention_vector_score')(att_layer)\n",
    "        # Reshape to obtain the same shape as input\n",
    "        att_layer = Permute((2, 1))(\n",
    "            RepeatVector(feature_vector_size)(att_layer))\n",
    "        layers = merge([att_layer, layers],  mode='mul')\n",
    "        return layers \n",
    "    \n",
    "    def add_embedding_layer(self, layers):\n",
    "        layers = super(AttBiLSTM, self).add_embedding_layer(layers)        \n",
    "        return self.add_attention_block(layers)\n",
    "    \n",
    "    def attention_predict(self, input_sequences):\n",
    "        \"\"\"Classifies the input sequences and returns the attention score.\n",
    "\n",
    "        Args:\n",
    "            model: a Keras model\n",
    "            input_: a list of array representation of sentences.\n",
    "\n",
    "        Returns:\n",
    "            A tuple where the first element is the attention scores for each\n",
    "            sentence, and the second is the model predictions.\n",
    "        \"\"\"\n",
    "        layer = self.model.get_layer('attention_vector_score')\n",
    "        attention_model = Model(\n",
    "            inputs=self.model.input, outputs=[layer.output, self.model.output])\n",
    "        # The attention output is (batch_size, timesteps, features)\n",
    "        return attention_model.predict(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttBiLSTM(vocabulary_size=unique_words.shape[0],\n",
    "               max_sentence_length=max_sentence_length,\n",
    "               labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/mteruel/miniconda3/envs/env_am/lib/python3.5/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23478 samples, validate on 5870 samples\n",
      "Epoch 1/2\n",
      "23478/23478 [==============================] - 371s 16ms/step - loss: 0.1275 - acc: 0.9765 - val_loss: 0.0298 - val_acc: 0.9911\n",
      "Epoch 2/2\n",
      " 4896/23478 [=====>........................] - ETA: 4:38 - loss: 0.0272 - acc: 0.9916"
     ]
    }
   ],
   "source": [
    "size = 100\n",
    "model.fit(X_train, numpy.array(y_train), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save('model_10ep_att2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention, predictions = model.attention_predict(numpy.array(X_test[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 140)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second attention model\n",
    "\n",
    "We implement the Philippe Remy model, to compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttBiLSTM2(BiLSTM):\n",
    "    \n",
    "    def add_attention_block(self, layers):\n",
    "        \"\"\"Apply an attention block to a partial model layers.\"\"\"\n",
    "        timesteps = K.int_shape(layers)[-2]\n",
    "        att_layer = Permute((2, 1))(att_layer)\n",
    "        att_layer = TimeDistributed(\n",
    "            Dense(timesteps, activation=None),\n",
    "            name='attention_matrix_score')(att_layer)\n",
    "        # Calculate a single score for each timestep\n",
    "        att_layer = Lambda(lambda x: K.mean(x, axis=2),\n",
    "                           name='attention_vector_score')(att_layer)\n",
    "        # Reshape to obtain the same shape as input\n",
    "        att_layer = Permute((2, 1))(\n",
    "            RepeatVector(feature_vector_size)(att_layer))\n",
    "        layers = merge([att_layer, layers],  mode='mul')\n",
    "        return layers \n",
    "    \n",
    "    def add_embedding_layer(self, layers):\n",
    "        layers = super(AttBiLSTM, self).add_embedding_layer(layers)        \n",
    "        return self.add_attention_block(layers)\n",
    "    \n",
    "    def attention_predict(self, input_sequences):\n",
    "        \"\"\"Classifies the input sequences and returns the attention score.\n",
    "\n",
    "        Args:\n",
    "            model: a Keras model\n",
    "            input_: a list of array representation of sentences.\n",
    "\n",
    "        Returns:\n",
    "            A tuple where the first element is the attention scores for each\n",
    "            sentence, and the second is the model predictions.\n",
    "        \"\"\"\n",
    "        layer = self.model.get_layer('attention_vector_score')\n",
    "        attention_model = Model(\n",
    "            inputs=self.model.input, outputs=[layer.output, self.model.output])\n",
    "        # The attention output is (batch_size, timesteps, features)\n",
    "        return attention_model.predict(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100\n",
    "model.fit(X_train, numpy.array(y_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save('model_10ep_att3.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Visualize the attention\n",
    "\n",
    "First, we align the attention and labels output from the network, and remove all the padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "# This could be done in a much more compact code, but I hope this is more\n",
    "# understandable\n",
    "for sentence_idx, (word_idxs, sentence_a_scores, sentence_labels) in enumerate(\n",
    "        zip(X_test[0:2], attention, numpy.argmax(predictions, axis=-1))):\n",
    "    for word_idx, a_score, label_idx in zip(word_idxs, sentence_a_scores, sentence_labels):\n",
    "        word = unique_words[word_idx]\n",
    "        if word == 'ENDPAD':\n",
    "            break\n",
    "        label = labels[label_idx]\n",
    "        result.append((word, a_score, sentence_idx, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Health', -0.52677673, 0, 'O'), ('officials', 2.1964862, 0, 'O')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a stand alone service, we first must store the results in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas.DataFrame(result, columns=['token', 'attention', 'sentence', 'label']).to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the file, you need to run a local http server to see the result. Run in the console from the repository directory:\n",
    "\n",
    "$ python -m http.server\n",
    "\n",
    "Then, open your browser in localhost:8000, and you should see the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualizing attention in notebook\n",
    "\n",
    "Another option is to import d3 directly into the notebook, but it is less robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from string import Template\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"js/d3.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<script src=\"js/d3.min.js\"></script>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"js/textChart.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<script src=\"js/textChart.js\"></script>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "if (d3 === undefined) {\n",
       "    alert('No d3 library');\n",
       "}\n",
       "if (TextChart === undefined) {\n",
       "    alert('No Chart library');\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"<script>\n",
    "if (d3 === undefined) {\n",
    "    alert('No d3 library');\n",
    "}\n",
    "if (TextChart === undefined) {\n",
    "    alert('No Chart library');\n",
    "}\n",
    "</script>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = pandas.DataFrame(\n",
    "    result, columns=['token', 'attention', 'sentence', 'label']).to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id='text-container'></div>\n",
       "    <script>\n",
       "var nouns = [{\"token\":\"Health\",\"attention\":-0.526776731,\"sentence\":0,\"label\":\"O\"},{\"token\":\"officials\",\"attention\":2.1964862347,\"sentence\":0,\"label\":\"O\"},{\"token\":\"in\",\"attention\":-3.0508413315,\"sentence\":0,\"label\":\"O\"},{\"token\":\"Indonesia\",\"attention\":-4.4284124374,\"sentence\":0,\"label\":\"geo\"},{\"token\":\"say\",\"attention\":1.3597706556,\"sentence\":0,\"label\":\"O\"},{\"token\":\"local\",\"attention\":-2.3108584881,\"sentence\":0,\"label\":\"O\"},{\"token\":\"tests\",\"attention\":-3.6230487823,\"sentence\":0,\"label\":\"O\"},{\"token\":\"show\",\"attention\":1.5192821026,\"sentence\":0,\"label\":\"O\"},{\"token\":\"an\",\"attention\":3.6158707142,\"sentence\":0,\"label\":\"O\"},{\"token\":\"Indonesian\",\"attention\":-4.364317894,\"sentence\":0,\"label\":\"gpe\"},{\"token\":\"man\",\"attention\":3.2517747879,\"sentence\":0,\"label\":\"O\"},{\"token\":\"who\",\"attention\":-2.7091934681,\"sentence\":0,\"label\":\"O\"},{\"token\":\"died\",\"attention\":-2.8228013515,\"sentence\":0,\"label\":\"O\"},{\"token\":\"last\",\"attention\":2.9431526661,\"sentence\":0,\"label\":\"O\"},{\"token\":\"week\",\"attention\":-4.6386590004,\"sentence\":0,\"label\":\"O\"},{\"token\":\"was\",\"attention\":2.6260821819,\"sentence\":0,\"label\":\"O\"},{\"token\":\"infected\",\"attention\":1.8219109774,\"sentence\":0,\"label\":\"O\"},{\"token\":\"with\",\"attention\":-1.8395367861,\"sentence\":0,\"label\":\"O\"},{\"token\":\"the\",\"attention\":2.1367657185,\"sentence\":0,\"label\":\"O\"},{\"token\":\"bird\",\"attention\":4.7380723953,\"sentence\":0,\"label\":\"O\"},{\"token\":\"flu\",\"attention\":1.6460897923,\"sentence\":0,\"label\":\"O\"},{\"token\":\"virus\",\"attention\":1.6390814781,\"sentence\":0,\"label\":\"O\"},{\"token\":\".\",\"attention\":-4.4272756577,\"sentence\":0,\"label\":\"O\"},{\"token\":\"The\",\"attention\":1.5802291632,\"sentence\":1,\"label\":\"O\"},{\"token\":\"ban\",\"attention\":-2.9346754551,\"sentence\":1,\"label\":\"O\"},{\"token\":\",\",\"attention\":-1.9875285625,\"sentence\":1,\"label\":\"O\"},{\"token\":\"reported\",\"attention\":1.3927556276,\"sentence\":1,\"label\":\"O\"},{\"token\":\"Wednesday\",\"attention\":-6.9224524498,\"sentence\":1,\"label\":\"tim\"},{\"token\":\",\",\"attention\":-1.9875285625,\"sentence\":1,\"label\":\"O\"},{\"token\":\"was\",\"attention\":2.6260821819,\"sentence\":1,\"label\":\"O\"},{\"token\":\"issued\",\"attention\":-2.4803924561,\"sentence\":1,\"label\":\"O\"},{\"token\":\"by\",\"attention\":-1.7616087198,\"sentence\":1,\"label\":\"O\"},{\"token\":\"the\",\"attention\":2.1367657185,\"sentence\":1,\"label\":\"O\"},{\"token\":\"General\",\"attention\":-1.2983660698,\"sentence\":1,\"label\":\"org\"},{\"token\":\"Administration\",\"attention\":-2.0372476578,\"sentence\":1,\"label\":\"org\"},{\"token\":\"of\",\"attention\":-1.365200758,\"sentence\":1,\"label\":\"org\"},{\"token\":\"Press\",\"attention\":-0.5248449445,\"sentence\":1,\"label\":\"org\"},{\"token\":\"and\",\"attention\":-1.7216192484,\"sentence\":1,\"label\":\"O\"},{\"token\":\"Publication\",\"attention\":-0.3472368419,\"sentence\":1,\"label\":\"org\"},{\"token\":\",\",\"attention\":-1.9875285625,\"sentence\":1,\"label\":\"O\"},{\"token\":\"the\",\"attention\":2.1367657185,\"sentence\":1,\"label\":\"O\"},{\"token\":\"governing\",\"attention\":2.9225654602,\"sentence\":1,\"label\":\"O\"},{\"token\":\"body\",\"attention\":-2.1586256027,\"sentence\":1,\"label\":\"O\"},{\"token\":\"for\",\"attention\":-1.4743545055,\"sentence\":1,\"label\":\"O\"},{\"token\":\"written\",\"attention\":-2.4068210125,\"sentence\":1,\"label\":\"O\"},{\"token\":\"publications\",\"attention\":-0.9706159234,\"sentence\":1,\"label\":\"O\"},{\"token\":\".\",\"attention\":-4.4272756577,\"sentence\":1,\"label\":\"O\"},{\"token\":\"The\",\"attention\":1.5802291632,\"sentence\":1,\"label\":\"O\"},{\"token\":\"ban\",\"attention\":-2.9346754551,\"sentence\":1,\"label\":\"O\"},{\"token\":\",\",\"attention\":-1.9875285625,\"sentence\":1,\"label\":\"O\"},{\"token\":\"reported\",\"attention\":1.3927556276,\"sentence\":1,\"label\":\"O\"},{\"token\":\"Wednesday\",\"attention\":-6.9224524498,\"sentence\":1,\"label\":\"tim\"},{\"token\":\",\",\"attention\":-1.9875285625,\"sentence\":1,\"label\":\"O\"},{\"token\":\"was\",\"attention\":2.6260821819,\"sentence\":1,\"label\":\"O\"},{\"token\":\"issued\",\"attention\":-2.4803924561,\"sentence\":1,\"label\":\"O\"},{\"token\":\"by\",\"attention\":-1.7616087198,\"sentence\":1,\"label\":\"O\"},{\"token\":\"the\",\"attention\":2.1367657185,\"sentence\":1,\"label\":\"O\"},{\"token\":\"General\",\"attention\":-1.2983660698,\"sentence\":1,\"label\":\"org\"},{\"token\":\"Administration\",\"attention\":-2.0372476578,\"sentence\":1,\"label\":\"org\"},{\"token\":\"of\",\"attention\":-1.365200758,\"sentence\":1,\"label\":\"org\"},{\"token\":\"Press\",\"attention\":-0.5248449445,\"sentence\":1,\"label\":\"org\"},{\"token\":\"and\",\"attention\":-1.7216192484,\"sentence\":1,\"label\":\"O\"},{\"token\":\"Publication\",\"attention\":-0.3472368419,\"sentence\":1,\"label\":\"org\"},{\"token\":\",\",\"attention\":-1.9875285625,\"sentence\":1,\"label\":\"O\"},{\"token\":\"the\",\"attention\":2.1367657185,\"sentence\":1,\"label\":\"O\"},{\"token\":\"governing\",\"attention\":2.9225654602,\"sentence\":1,\"label\":\"O\"},{\"token\":\"body\",\"attention\":-2.1586256027,\"sentence\":1,\"label\":\"O\"},{\"token\":\"for\",\"attention\":-1.4743545055,\"sentence\":1,\"label\":\"O\"},{\"token\":\"written\",\"attention\":-2.4068210125,\"sentence\":1,\"label\":\"O\"},{\"token\":\"publications\",\"attention\":-0.9706159234,\"sentence\":1,\"label\":\"O\"},{\"token\":\".\",\"attention\":-4.4272756577,\"sentence\":1,\"label\":\"O\"}];  // We are heavily using the similarties\n",
       "                         // between js and json syntax.\n",
       "opts = {\n",
       "  lineHeight: 16,\n",
       "  width: 900,\n",
       "  height: 600,\n",
       "  linePadding: 10\n",
       "}\n",
       "chart = new TextChart(nouns, opts);\n",
       "chart.draw(\"text-container\");\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_text_template = Template('''\n",
    "var nouns = $json_data;  // We are heavily using the similarties\n",
    "                         // between js and json syntax.\n",
    "opts = {\n",
    "  lineHeight: 16,\n",
    "  width: 900,\n",
    "  height: 600,\n",
    "  linePadding: 10\n",
    "}\n",
    "chart = new TextChart(nouns, opts);\n",
    "chart.draw(\"text-container\");\n",
    "''')\n",
    "\n",
    "html_template = Template('''\n",
    "    <div id='text-container'></div>\n",
    "    <script>$js_text</script>\n",
    "''')\n",
    "\n",
    "js_text = js_text_template.substitute({\n",
    "    'json_data': json_data\n",
    "})\n",
    "\n",
    "HTML(html_template.substitute({'js_text': js_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_am]",
   "language": "python",
   "name": "conda-env-env_am-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
